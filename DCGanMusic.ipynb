{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGanMusic.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNY/3Kc93GhPBptQfY1LVwv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DRIP-AI-RESEARCH-JUNIOR/MUSIC_GENEARATION/blob/master/DCGanMusic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VICLgoD5KzNW",
        "outputId": "62f4ebca-2cc6-42eb-fe22-ea8c9ca6466f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZElgUeWDG2A"
      },
      "source": [
        "!cp -r /content/drive/My\\ Drive/midi /content"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9-9VRv4Kyak"
      },
      "source": [
        "!cp -r /content/drive/My\\ Drive/Nottingham /content"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqGpFWzTCs_R"
      },
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "sys.path.append('midi')\n",
        "import torch.utils.data as data\n",
        "from midi_utils import midiread, midiwrite\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "from matplotlib import pyplot as plt\n",
        "import skimage.io as io\n",
        "from IPython.display import FileLink"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TD_OcpkCzyW"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import torch.utils.data as data"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBbkDflMDkAu"
      },
      "source": [
        "# DATALOADER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNmPJmIkDZGn"
      },
      "source": [
        "def midi_filename_to_piano_roll(midi_filename):\n",
        "    \n",
        "    midi_data = midiread(midi_filename, dt=0.3)\n",
        "    \n",
        "    piano_roll = midi_data.piano_roll.transpose()\n",
        "    \n",
        "    # Pressed notes are replaced by 1\n",
        "    piano_roll[piano_roll > 0] = 1\n",
        "    \n",
        "    return piano_roll\n",
        " \n",
        " \n",
        "def pad_piano_roll(piano_roll, max_length=132333, pad_value=0):\n",
        "        \n",
        "    original_piano_roll_length = piano_roll.shape[1]\n",
        "    \n",
        "    padded_piano_roll = np.zeros((88, max_length))\n",
        "    padded_piano_roll[:] = pad_value\n",
        "    \n",
        "    padded_piano_roll[:, -original_piano_roll_length:] = piano_roll\n",
        " \n",
        "    return padded_piano_roll\n",
        " \n",
        " "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVdnxZM8Dm5c"
      },
      "source": [
        " class NotesGenerationDataset(data.Dataset):\n",
        "    \n",
        "    def __init__(self, midi_folder_path, longest_sequence_length=1491):\n",
        "        \n",
        "        self.midi_folder_path = midi_folder_path\n",
        "        \n",
        "        midi_filenames = os.listdir(midi_folder_path)\n",
        "        \n",
        "        self.longest_sequence_length = longest_sequence_length\n",
        "        \n",
        "        midi_full_filenames = map(lambda filename: os.path.join(midi_folder_path, filename),midi_filenames)\n",
        "        \n",
        "        self.midi_full_filenames = list(midi_full_filenames)\n",
        "        \n",
        "        if longest_sequence_length is None:\n",
        "            \n",
        "            self.update_the_max_length()\n",
        "    \n",
        "    \n",
        "    def update_the_max_length(self):\n",
        "        \n",
        "        sequences_lengths = map(lambda filename: midi_filename_to_piano_roll(filename).shape[1],self.midi_full_filenames)\n",
        "        \n",
        "        max_length = max(sequences_lengths)\n",
        "        \n",
        "        self.longest_sequence_length = max_length\n",
        "                \n",
        "    \n",
        "    def __len__(self):\n",
        "        \n",
        "        return len(self.midi_full_filenames)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        midi_full_filename = self.midi_full_filenames[index]\n",
        "        \n",
        "        piano_roll = midi_filename_to_piano_roll(midi_full_filename)\n",
        "\n",
        "        # padding sequence so that all of them have the same length\n",
        "        input_sequence_padded = pad_piano_roll(piano_roll, max_length=self.longest_sequence_length)\n",
        "        # print(input_sequence_padded.shape)\n",
        "                \n",
        "        input_sequence_padded = input_sequence_padded.transpose()\n",
        "        input_sequence_padded = torch.FloatTensor(input_sequence_padded).unsqueeze(0)\n",
        "\n",
        "        return input_sequence_padded"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3vzu9-3Lkyb"
      },
      "source": [
        "trainset = NotesGenerationDataset('Nottingham/train/', longest_sequence_length=None)\n",
        " \n",
        "train_loader = data.DataLoader(trainset, batch_size=4,shuffle=True, drop_last=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSAmK26ZMEUG",
        "outputId": "92486944-d372-4870-df97-1a31b38ba578"
      },
      "source": [
        "a = next(iter(train_loader))\n",
        "print(a.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8, 1, 1491, 88])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zepIK3M0OqhG"
      },
      "source": [
        "# UTILS FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnBReFvnOtZQ"
      },
      "source": [
        "def conv_cond_concat(x, y):\n",
        "    \"\"\"Concatenate conditioning vector on feature map axis.\"\"\"\n",
        "    x_shapes = x.shape\n",
        "    y_shapes = y.shape\n",
        "    y2 = y.expand(x_shapes[0],y_shapes[1],x_shapes[2],x_shapes[3])\n",
        "\n",
        "    return torch.cat((x, y2),1)\n",
        "\n",
        "def conv_prev_concat(x, y):\n",
        "    \"\"\"Concatenate conditioning vector on feature map axis.\"\"\"\n",
        "    x_shapes = x.shape\n",
        "    y_shapes = y.shape\n",
        "    if x_shapes[2:] == y_shapes[2:]:\n",
        "        y2 = y.expand(x_shapes[0],y_shapes[1],x_shapes[2],x_shapes[3])\n",
        "\n",
        "        return torch.cat((x, y2),1)\n",
        "\n",
        "    else:\n",
        "        print(x_shapes[2:])\n",
        "        print(y_shapes[2:])\n",
        "\n",
        "\n",
        "\n",
        "def batch_norm_1d(x):\n",
        "    x_shape = x.shape[1]\n",
        "    batch_nor = nn.BatchNorm1d(x_shape, eps=1e-05, momentum=0.9, affine=True)\n",
        "    batch_nor = batch_nor.cuda()\n",
        "\n",
        "    output = batch_nor(x)\n",
        "    return output\n",
        "\n",
        "\n",
        "def batch_norm_1d_cpu(x):\n",
        "    x_shape = x.shape[1]\n",
        "    # ipdb.set_trace()\n",
        "    # batch_nor = nn.BatchNorm1d(x_shape, eps=1e-05, momentum=0.9, affine=True)\n",
        "    # output = batch_nor(x)\n",
        "    output = x\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def batch_norm_2d(x):\n",
        "    x_shape = x.shape[1]\n",
        "    batch_nor = nn.BatchNorm2d(x_shape, eps=1e-05, momentum=0.9, affine=True)\n",
        "    batch_nor = batch_nor.cuda()\n",
        "    output = batch_nor(x)\n",
        "    return output\n",
        "\n",
        "\n",
        "def batch_norm_2d_cpu(x):\n",
        "    # x_shape = x.shape[1]\n",
        "    # batch_nor = nn.BatchNorm2d(x_shape, eps=1e-05, momentum=0.9, affine=True)\n",
        "    # batch_nor = batch_nor\n",
        "    # output = batch_nor(x)\n",
        "    output = x\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "def sigmoid_cross_entropy_with_logits(inputs,labels):\n",
        "    loss = nn.BCEWithLogitsLoss()\n",
        "    output = loss(inputs, labels)\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "def reduce_mean(x):\n",
        "    output = torch.mean(x,0, keepdim = False)\n",
        "    output = torch.mean(output,-1, keepdim = False)\n",
        "    return output\n",
        "\n",
        "\n",
        "def reduce_mean_0(x):\n",
        "    output = torch.mean(x,0, keepdim = False)\n",
        "    return output\n",
        "\n",
        "\n",
        "def l2_loss(x,y):\n",
        "    loss_ = nn.MSELoss(reduction='sum')\n",
        "    l2_loss_ = loss_(x, y)/2\n",
        "    return l2_loss_\n",
        "\n",
        "\n",
        "\n",
        "def lrelu(x, leak=0.2):\n",
        "    z = torch.mul(x,leak)\n",
        "    return torch.max(x, z)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohi7DEzsMiiQ"
      },
      "source": [
        "# DISCRIMINATOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qte6MIOdY0ha"
      },
      "source": [
        "# Batch size during training\n",
        "batch_size = 4\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this\n",
        "#   size using a transformer.\n",
        "image_size = 64\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 1\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "\n",
        "# Size of feature maps in generator\n",
        "ngf = 16\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 16\n",
        "\n",
        "# Number of training epochs\n",
        "epochs = 5\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 0.0002\n",
        "\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.5\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP0u97ylZAtp"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        nc = 1\n",
        "        ndf = 16\n",
        "        self.layer = nn.Sequential(\n",
        "            # input is 1 x 1491 x 88\n",
        "            nn.Conv2d(nc, ndf, (11,3), 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf, ndf * 2, (11,3), 3, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, (7,3), 3, 2, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, (9,3), 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 8, 1, (7,5), 4, 1, bias=False),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.linear = nn.Conv2d(1, 1, (9,1),1,0,bias=False)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.layer(input)\n",
        "        x = self.linear(x)\n",
        "        out = self.sig(x)\n",
        "        return out,x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFkWWYzJZgbx",
        "outputId": "926c51d5-56fa-4f83-9089-f20f67fe9c64"
      },
      "source": [
        "inp = torch.randn(128,1,1491,88)\n",
        "d = Discriminator()\n",
        "out,x = d(inp)\n",
        "print(out.shape,x.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 1, 1, 1]) torch.Size([128, 1, 1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzDFS9IzNhHD"
      },
      "source": [
        "# GENERATOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saKEMUT6kmvc"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d( nz, 1, (9,1),1,0,bias=False),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(1, ngf * 8, (8,5), 4, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, (9,4), 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d( ngf * 4, ngf * 2, (9,4), 3, 2, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d( ngf * 2, ngf, (12,4), 3, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d( ngf, nc,  (11,4), 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "        # self.conv1 = nn.ConvTranspose2d( nz, 1, (9,1),1,0,bias=False)\n",
        "        # self.conv2 = nn.ConvTranspose2d( 1, ngf * 8, (8,5), 4, 1, bias=False)\n",
        "        # self.conv3 = nn.ConvTranspose2d(ngf * 8, ngf * 4, (9,4), 2, 1, bias=False)\n",
        "        # self.conv4 = nn.ConvTranspose2d( ngf * 4, ngf * 2, (9,4), 3, 2, bias=False)\n",
        "        # self.conv5 = nn.ConvTranspose2d( ngf * 2, ngf, (12,4), 3, 1, bias=False)\n",
        "        # self.conv6 = nn.ConvTranspose2d( ngf, nc,  (11,4), 2, 1, bias=False)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # print('1',input.shape)\n",
        "        # x = self.conv1(input)\n",
        "        # print('2',x.shape)\n",
        "        # x = self.conv2(x)\n",
        "        # print('3',x.shape)\n",
        "        # x = self.conv3(x)\n",
        "        # print('4',x.shape)\n",
        "        # x = self.conv4(x)\n",
        "        # print('5',x.shape)\n",
        "        # x = self.conv5(x)\n",
        "        # print('6',x.shape)\n",
        "        # x = self.conv6(x)\n",
        "        # return x\n",
        "        return self.main(input)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x5PqNqKPBkN",
        "outputId": "2f7fdee9-4e60-4351-f843-58c9b71ec7fa"
      },
      "source": [
        "net_g = Generator()\n",
        "z = torch.randn(128,100,1,1)\n",
        "out = net_g(z)\n",
        "print(out.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 1, 1491, 88])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWCtT_9eO_G2"
      },
      "source": [
        "# testing model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N43YC8k1N2fl"
      },
      "source": [
        "# SANPLE GENERATOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4045uaRNxpN"
      },
      "source": [
        "class sample_generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(sample_generator, self).__init__()\n",
        "        self.gf_dim   = 64\n",
        "        self.y_dim   = 13\n",
        "        self.n_channel = 256\n",
        "\n",
        "        self.h1      = nn.ConvTranspose2d(in_channels=157, out_channels=pitch_range, kernel_size=(2,1), stride=(2,2))\n",
        "        self.h2      = nn.ConvTranspose2d(in_channels=157, out_channels=pitch_range, kernel_size=(2,1), stride=(2,2))\n",
        "        self.h3      = nn.ConvTranspose2d(in_channels=157, out_channels=pitch_range, kernel_size=(2,1), stride=(2,2))\n",
        "        self.h4      = nn.ConvTranspose2d(in_channels=157, out_channels=1, kernel_size=(1,pitch_range), stride=(1,2))\n",
        "\n",
        "        self.h0_prev = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(1,pitch_range), stride=(1,2))\n",
        "        self.h1_prev = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(2,1), stride=(2,2))\n",
        "        self.h2_prev = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(2,1), stride=(2,2))\n",
        "        self.h3_prev = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(2,1), stride=(2,2))\n",
        "\n",
        "        self.linear1 = nn.Linear(113,1024)\n",
        "        self.linear2 = nn.Linear(1037,self.gf_dim*2*2*1)\n",
        "\n",
        "    def forward(self, z, prev_x, y ,batch_size,pitch_range):\n",
        "\n",
        "        # h3_prev = F.leaky_relu(self.batch_nor_256(self.h0_prev(prev_x)),0.2)\n",
        "        h0_prev = lrelu(batch_norm_2d_cpu(self.h0_prev(prev_x)),0.2)   #[72, 16, 16, 1]\n",
        "        h1_prev = lrelu(batch_norm_2d_cpu(self.h1_prev(h0_prev)),0.2)  #[72, 16, 8, 1]\n",
        "        h2_prev = lrelu(batch_norm_2d_cpu(self.h2_prev(h1_prev)),0.2)  #[72, 16, 4, 1]\n",
        "        h3_prev = lrelu(batch_norm_2d_cpu(self.h3_prev(h2_prev)),0.2)  #[72, 16, 2, 1])\n",
        "\n",
        "        yb = y.view(batch_size,  self.y_dim, 1, 1)  #(72,13,1,1)\n",
        "\n",
        "        z = torch.cat((z,y),1)         #(72,113)\n",
        "\n",
        "        h0 = F.relu(batch_norm_1d_cpu(self.linear1(z)))    #(72,1024)\n",
        "        h0 = torch.cat((h0,y),1)   #(72,1037)\n",
        "\n",
        "        h1 = F.relu(batch_norm_1d_cpu(self.linear2(h0)))   #(72, 256)\n",
        "        h1 = h1.view(batch_size, self.gf_dim * 2, 2, 1)     #(72,128,2,1)\n",
        "        h1 = conv_cond_concat(h1,yb) #(b,141,2,1)\n",
        "        h1 = conv_prev_concat(h1,h3_prev)  #(72, 157, 2, 1)\n",
        "\n",
        "        h2 = F.relu(batch_norm_2d_cpu(self.h1(h1)))  #(72, 128, 4, 1)\n",
        "        h2 = conv_cond_concat(h2,yb) #([72, 141, 4, 1])\n",
        "        h2 = conv_prev_concat(h2,h2_prev)  #([72, 157, 4, 1])\n",
        "\n",
        "        h3 = F.relu(batch_norm_2d_cpu(self.h2(h2)))  #([72, 128, 8, 1]) \n",
        "        h3 = conv_cond_concat(h3,yb)  #([72, 141, 8, 1])\n",
        "        h3 = conv_prev_concat(h3,h1_prev) #([72, 157, 8, 1])\n",
        "\n",
        "        h4 = F.relu(batch_norm_2d_cpu(self.h3(h3)))  #([72, 128, 16, 1])\n",
        "        h4 = conv_cond_concat(h4,yb)  #([72, 141, 16, 1])\n",
        "        h4 = conv_prev_concat(h4,h0_prev) #([72, 157, 16, 1])\n",
        "\n",
        "        g_x = torch.sigmoid(self.h4(h4)) #([72, 1, 16, 128])\n",
        "\n",
        "        return g_x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AToAjuZWON-m"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t00bhgEPOQuk"
      },
      "source": [
        "def train(netG, netD, optimizerG, optimizerD,criterion ,epochs,dataloader, batch_size, nz, device=torch.device('cuda')):\n",
        "  \n",
        "  fixed_noise = torch.randn(batch_size, nz, device=device)\n",
        "  netD.train()\n",
        "  netG.train()\n",
        "  \n",
        "  real_label = 1\n",
        "  fake_label = 0\n",
        "  average_lossD = 0\n",
        "  average_lossG = 0\n",
        "  average_D_x   = 0\n",
        "  average_D_G_z = 0\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    sum_lossD = 0\n",
        "    sum_lossG = 0\n",
        "    sum_D_x   = 0\n",
        "    sum_D_G_z = 0\n",
        "    average_lossD = 0\n",
        "    average_lossG = 0\n",
        "    average_D_x   = 0\n",
        "    average_D_G_z = 0\n",
        "    \n",
        "    for i, (data) in enumerate(train_loader, 0):\n",
        "      \n",
        "      #############################################################\n",
        "      # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "      #############################################################\n",
        "      \n",
        "      # train with real      \n",
        "      netD.zero_grad()\n",
        "      real_cpu = data.to(device)\n",
        "      \n",
        "      batch_size = real_cpu.size()[0]\n",
        "      label = torch.full((batch_size,), real_label,dtype=torch.float, device=device)\n",
        "      D, D_logits = netD(real_cpu)\n",
        "      \n",
        "      #####loss\n",
        "      d_loss_real = reduce_mean(sigmoid_cross_entropy_with_logits(D_logits, 0.9*torch.ones_like(D)))\n",
        "      d_loss_real.backward(retain_graph=True)\n",
        "      D_x = D.mean().item()\n",
        "      sum_D_x += D_x \n",
        "      \n",
        "      # train with fake\n",
        "      noise = torch.randn(batch_size, nz,1,1, device=device)\n",
        "      fake = netG(noise)\n",
        "      label.fill_(fake_label)\n",
        "      D_, D_logits_ = netD(fake.detach())\n",
        "      d_loss_fake = reduce_mean(sigmoid_cross_entropy_with_logits(D_logits_, torch.zeros_like(D_)))\n",
        "      \n",
        "      d_loss_fake.backward(retain_graph=True)\n",
        "      D_G_z1 = D_.mean().item()\n",
        "      errD = d_loss_real + d_loss_fake\n",
        "      errD = errD.item()\n",
        "      sum_lossD += errD\n",
        "      optimizerD.step()\n",
        "      \n",
        "      #############################################\n",
        "      # (2) Update G network: maximize log(D(G(z)))\n",
        "      #############################################\n",
        "      \n",
        "      netG.zero_grad()\n",
        "      label.fill_(real_label)  # fake labels are real for generator cost\n",
        "      D_, D_logits_= netD(fake)\n",
        "      \n",
        "      ###loss\n",
        "      errG = reduce_mean(sigmoid_cross_entropy_with_logits(D_logits_, torch.ones_like(D_)))\n",
        "\n",
        "      errG.backward(retain_graph=True)\n",
        "      sum_lossG +=errG\n",
        "      D_G_z2 = D_.mean().item()\n",
        "      sum_D_G_z += D_G_z2\n",
        "      optimizerG.step()\n",
        "  \n",
        "    average_lossD = (sum_lossD / len(train_loader))\n",
        "    average_lossG = (sum_lossG / len(train_loader))\n",
        "    average_D_x = (sum_D_x / len(train_loader))\n",
        "    average_D_G_z = (sum_D_G_z / len(train_loader))\n",
        "  \n",
        "    # lossD_list.append(average_lossD)\n",
        "    # lossG_list.append(average_lossG)\n",
        "    # D_x_list.append(average_D_x)\n",
        "    # D_G_z_list.append(average_D_G_z)\n",
        "  \n",
        "    print('==> Epoch: {} Average lossD: {:.10f} average_lossG: {:.10f},average D(x): {:.10f},average D(G(z)): {:.10f} '.format(\n",
        "     epoch, average_lossD,average_lossG,average_D_x, average_D_G_z))\n",
        "    del average_lossD,average_lossG,data,average_D_x,real_cpu,average_D_G_z,sum_lossD,sum_lossG,sum_D_x,sum_D_G_z,label,noise,fake,D,D_,D_logits,D_logits_,d_loss_fake,d_loss_real\n",
        "    torch.cuda.empty_cache()\n",
        "  "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcNNHY6ZOTqZ"
      },
      "source": [
        "def main():\n",
        "  epochs = 50\n",
        "  lr = 0.0002\n",
        "  \n",
        "  device = torch.device('cuda')\n",
        "  \n",
        "  netG = Generator().to(device)\n",
        "  netD = Discriminator().to(device)\n",
        "\n",
        "  criterion = nn.BCELoss()\n",
        "  \n",
        "  optimizerD = optim.Adam(netD.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
        "  optimizerG = optim.Adam(netG.parameters(), lr=0.01, betas=(0.5, 0.999)) \n",
        "  train(netG, netD, optimizerG, optimizerD,criterion ,epochs,train_loader,batch_size, nz, device=device)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu0uUce04ojG",
        "outputId": "3dd5bdc2-798e-494f-d137-215875ba6489"
      },
      "source": [
        "main()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Epoch: 0 Average lossD: 0.8903129959 average_lossG: 1.1008552313,average D(x): 0.7327624298,average D(G(z)): 0.3446985123 \n",
            "==> Epoch: 1 Average lossD: 0.4859435402 average_lossG: 2.0230648518,average D(x): 0.8898300408,average D(G(z)): 0.1384144361 \n",
            "==> Epoch: 2 Average lossD: 0.3993730983 average_lossG: 2.9117200375,average D(x): 0.8949334318,average D(G(z)): 0.0566387063 \n",
            "==> Epoch: 3 Average lossD: 0.3763580059 average_lossG: 3.5685777664,average D(x): 0.8950791486,average D(G(z)): 0.0311852932 \n",
            "==> Epoch: 4 Average lossD: 0.3477070602 average_lossG: 4.1130576134,average D(x): 0.8989750846,average D(G(z)): 0.0164908592 \n",
            "==> Epoch: 5 Average lossD: 0.3375416052 average_lossG: 4.6302423477,average D(x): 0.8999665827,average D(G(z)): 0.0098784929 \n",
            "==> Epoch: 6 Average lossD: 0.3333091135 average_lossG: 5.0038037300,average D(x): 0.9001691486,average D(G(z)): 0.0067403686 \n",
            "==> Epoch: 7 Average lossD: 0.3316283386 average_lossG: 5.3023233414,average D(x): 0.8997073311,average D(G(z)): 0.0049955884 \n",
            "==> Epoch: 8 Average lossD: 0.3302076481 average_lossG: 5.5597195625,average D(x): 0.8999360476,average D(G(z)): 0.0038601008 \n",
            "==> Epoch: 9 Average lossD: 0.3290580861 average_lossG: 5.7980575562,average D(x): 0.9000161652,average D(G(z)): 0.0030400836 \n",
            "==> Epoch: 10 Average lossD: 0.3281766183 average_lossG: 6.0210571289,average D(x): 0.8999489801,average D(G(z)): 0.0024319548 \n",
            "==> Epoch: 11 Average lossD: 0.3275852687 average_lossG: 6.2316026688,average D(x): 0.8999173514,average D(G(z)): 0.0019695770 \n",
            "==> Epoch: 12 Average lossD: 0.3272033148 average_lossG: 6.4247064590,average D(x): 0.8999894168,average D(G(z)): 0.0016233595 \n",
            "==> Epoch: 13 Average lossD: 0.3271499914 average_lossG: 6.6015772820,average D(x): 0.8999379953,average D(G(z)): 0.0013597700 \n",
            "==> Epoch: 14 Average lossD: 0.3265860861 average_lossG: 6.7705402374,average D(x): 0.8999288878,average D(G(z)): 0.0011483972 \n",
            "==> Epoch: 15 Average lossD: 0.3265981719 average_lossG: 6.9340620041,average D(x): 0.8999979803,average D(G(z)): 0.0009750054 \n",
            "==> Epoch: 16 Average lossD: 0.3264736611 average_lossG: 7.0803699493,average D(x): 0.8998973022,average D(G(z)): 0.0008421911 \n",
            "==> Epoch: 17 Average lossD: 0.3261420567 average_lossG: 7.2200856209,average D(x): 0.8999221535,average D(G(z)): 0.0007323980 \n",
            "==> Epoch: 18 Average lossD: 0.3261042842 average_lossG: 7.3652286530,average D(x): 0.8999867026,average D(G(z)): 0.0006333787 \n",
            "==> Epoch: 19 Average lossD: 0.3260607156 average_lossG: 7.4928007126,average D(x): 0.8999752705,average D(G(z)): 0.0005574502 \n",
            "==> Epoch: 20 Average lossD: 0.3260771761 average_lossG: 7.6104989052,average D(x): 0.8999084410,average D(G(z)): 0.0004954887 \n",
            "==> Epoch: 21 Average lossD: 0.3262304255 average_lossG: 7.7181372643,average D(x): 0.8999110967,average D(G(z)): 0.0004448198 \n",
            "==> Epoch: 22 Average lossD: 0.3275866634 average_lossG: 7.7391262054,average D(x): 0.8995724214,average D(G(z)): 0.0004365308 \n",
            "==> Epoch: 23 Average lossD: 0.3269798235 average_lossG: 7.7801208496,average D(x): 0.8997437513,average D(G(z)): 0.0004180352 \n",
            "==> Epoch: 24 Average lossD: 0.3261166773 average_lossG: 7.8626050949,average D(x): 0.8999585413,average D(G(z)): 0.0003850297 \n",
            "==> Epoch: 25 Average lossD: 0.3257647341 average_lossG: 7.9759612083,average D(x): 0.8999828400,average D(G(z)): 0.0003438292 \n",
            "==> Epoch: 26 Average lossD: 0.3256478005 average_lossG: 8.1004209518,average D(x): 0.8998989363,average D(G(z)): 0.0003036094 \n",
            "==> Epoch: 27 Average lossD: 0.3255969258 average_lossG: 8.2193660736,average D(x): 0.8999696787,average D(G(z)): 0.0002695454 \n",
            "==> Epoch: 28 Average lossD: 0.3255080016 average_lossG: 8.3384418488,average D(x): 0.8999923729,average D(G(z)): 0.0002392741 \n",
            "==> Epoch: 29 Average lossD: 0.4121032269 average_lossG: 7.2327923775,average D(x): 0.8813304702,average D(G(z)): 0.0136368566 \n",
            "==> Epoch: 30 Average lossD: 0.3345866236 average_lossG: 7.5486769676,average D(x): 0.9004181765,average D(G(z)): 0.0005284211 \n",
            "==> Epoch: 31 Average lossD: 0.3280767518 average_lossG: 7.5921931267,average D(x): 0.8999445714,average D(G(z)): 0.0005043852 \n",
            "==> Epoch: 32 Average lossD: 0.3274237727 average_lossG: 7.6163749695,average D(x): 0.9000844418,average D(G(z)): 0.0004923376 \n",
            "==> Epoch: 33 Average lossD: 0.3268959539 average_lossG: 7.6514897346,average D(x): 0.9000939207,average D(G(z)): 0.0004753732 \n",
            "==> Epoch: 34 Average lossD: 0.3266629447 average_lossG: 7.6971583366,average D(x): 0.9000982077,average D(G(z)): 0.0004541644 \n",
            "==> Epoch: 35 Average lossD: 0.3264257451 average_lossG: 7.7447896004,average D(x): 0.9000484272,average D(G(z)): 0.0004330478 \n",
            "==> Epoch: 36 Average lossD: 0.3262761433 average_lossG: 7.7986488342,average D(x): 0.9000378127,average D(G(z)): 0.0004103384 \n",
            "==> Epoch: 37 Average lossD: 0.3261859119 average_lossG: 7.8490271568,average D(x): 0.8999907243,average D(G(z)): 0.0003901781 \n",
            "==> Epoch: 38 Average lossD: 0.3259597400 average_lossG: 7.9109611511,average D(x): 0.8998729986,average D(G(z)): 0.0003667652 \n",
            "==> Epoch: 39 Average lossD: 0.3258399245 average_lossG: 7.9759397507,average D(x): 0.9000433656,average D(G(z)): 0.0003436971 \n",
            "==> Epoch: 40 Average lossD: 0.3257680196 average_lossG: 8.0455684662,average D(x): 0.8999823694,average D(G(z)): 0.0003205888 \n",
            "==> Epoch: 41 Average lossD: 0.3258794391 average_lossG: 8.1153678894,average D(x): 0.8998182743,average D(G(z)): 0.0002989590 \n",
            "==> Epoch: 42 Average lossD: 0.3257681631 average_lossG: 8.1711635590,average D(x): 0.9000100835,average D(G(z)): 0.0002827463 \n",
            "==> Epoch: 43 Average lossD: 0.3256914306 average_lossG: 8.2376432419,average D(x): 0.8999633737,average D(G(z)): 0.0002645717 \n",
            "==> Epoch: 44 Average lossD: 0.3256418707 average_lossG: 8.3087396622,average D(x): 0.8999558729,average D(G(z)): 0.0002464110 \n",
            "==> Epoch: 45 Average lossD: 0.3257561450 average_lossG: 8.3844900131,average D(x): 0.9000301678,average D(G(z)): 0.0002284246 \n",
            "==> Epoch: 46 Average lossD: 0.3255895855 average_lossG: 8.4368371964,average D(x): 0.8998773939,average D(G(z)): 0.0002167863 \n",
            "==> Epoch: 47 Average lossD: 0.3254933168 average_lossG: 8.5161275864,average D(x): 0.8999157524,average D(G(z)): 0.0002002664 \n",
            "==> Epoch: 48 Average lossD: 0.3254926353 average_lossG: 8.5987882614,average D(x): 0.9000235610,average D(G(z)): 0.0001843861 \n",
            "==> Epoch: 49 Average lossD: 0.3254942858 average_lossG: 8.6745243073,average D(x): 0.8999277808,average D(G(z)): 0.0001709232 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX6Y-9ZPC1m7"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qohk10gP72rY"
      },
      "source": [
        "def train(netG, netD, optimizerG, optimizerD,criterion ,epochs,dataloader, batch_size, nz, device=torch.device('cuda')):\n",
        "    img_list = []\n",
        "    G_losses = []\n",
        "    D_losses = []\n",
        "    iters = 0\n",
        "    real_label = 1.\n",
        "    fake_label = 0.\n",
        "\n",
        "    print(\"Starting Training Loop...\")\n",
        "    # For each epoch\n",
        "    for epoch in range(num_epochs):\n",
        "    # For each batch in the dataloader\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        ## Train with all-real batch\n",
        "            netD.zero_grad()\n",
        "        # Format batch\n",
        "            real_cpu = data.to(device)\n",
        "            b_size = real_cpu.size()[0]\n",
        "            label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "        # Forward pass real batch through D\n",
        "            output = netD(real_cpu).view(-1)\n",
        "        # Calculate loss on all-real batch\n",
        "            errD_real = criterion(output, label)\n",
        "        # Calculate gradients for D in backward pass\n",
        "            errD_real.backward()\n",
        "            D_x = output.mean().item()\n",
        "\n",
        "        ## Train with all-fake batch\n",
        "        # Generate batch of latent vectors\n",
        "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "        # Generate fake image batch with G\n",
        "            fake = netG(noise)\n",
        "            label.fill_(fake_label)\n",
        "        # Classify all fake batch with D\n",
        "            output = netD(fake.detach()).view(-1)\n",
        "        # Calculate D's loss on the all-fake batch\n",
        "            errD_fake = criterion(output, label)\n",
        "        # Calculate the gradients for this batch\n",
        "            errD_fake.backward()\n",
        "            D_G_z1 = output.mean().item()\n",
        "        # Add the gradients from the all-real and all-fake batches\n",
        "            errD = errD_real + errD_fake\n",
        "        # Update D\n",
        "            optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "            netG.zero_grad()\n",
        "            label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "            output = netD(fake).view(-1)\n",
        "        # Calculate G's loss based on this output\n",
        "            errG = criterion(output, label)\n",
        "        # Calculate gradients for G\n",
        "            errG.backward()\n",
        "            D_G_z2 = output.mean().item()\n",
        "        # Update G\n",
        "            optimizerG.step()\n",
        "\n",
        "        # Output training stats\n",
        "            if i % 50 == 0:\n",
        "                print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                      % (epoch, num_epochs, i, len(dataloader),\n",
        "                         errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "        # Save Losses for plotting later\n",
        "            G_losses.append(errG.item())\n",
        "            D_losses.append(errD.item())\n"
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}